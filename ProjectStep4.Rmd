---
title: '(F23) PSTAT 126: Project Step 4'
author: "Anthony Cu and William Mahnke"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      comment = NA)

library(tidyverse)
library(dplyr)
library(ggplot2)
library(tidymodels)
library(faraway)
library(glmnet)
library(randomForest)

# load data 
houseData <- read.csv("~/Desktop/Projects/PSTAT 126/Project/ProjectStep1/houseData.csv")
```

### Introduction
In our project, we explore the 1990 California Housing data set, providing information on a specified district in the state. The survey data describes homes in a district of California in 1990 in order to represent the larger population of this district during the 1990's overall. Each independent observation corresponds to a different block within the district. The data set comes from Kaggle's data repository.

We view the correlations between our explanatory variables. 
```{r correlations} 
# pairwise correlations 
round(cor(houseData[-c(1:3, 11:12)]), 2)
```

We begin by fitting a naive linear model to our dataset, by inputting all our variables of interest linearly. 
```{r correlated values, echo = T}
fit <- lm(median_house_value ~., data = houseData[-c(1:3, 12)])
```

```{r summary, results = "hide"}
summary(fit)
```
From this fitted model, we observe that $R^2$ = `r summary(fit)$r.squared`, which is moderately large. 

We now check the eigen decomposition of $x^Tx$
```{r eigendecomposition}
# we check the eigen decomposition of xTx
x <- model.matrix(fit)[,-1]
lambda <- eigen(crossprod(x))$val

# we calculate the R^2j for all the predictors
r2 <- rep(0, dim(x)[2])
for(k in 1:length(r2)) {
  r2[k] <- summary(lm(x[,k] ~ x[,-k]))$r.squared
}

r2
```

We obtain that $\sqrt{\frac{\lambda_1}{\lambda_p}}$ for each predictor is: 
```{r}
sqrt(lambda[1]/lambda)
```

Further, the $R^2_j$ for all predictors is:
```{r}
r2
```

We now check the variance inflation factors: 
```{r vif}
faraway::vif(x)
```

```{r fitted model2 after removal}
# Removing the highly correlated variables we are left with `housing_median_age` and `median_income`.
fit2 <- lm(median_house_value ~ housing_median_age + median_income, data = houseData[-c(1:3, 12)])
```

```{r, results = "hide"}
summary(fit2)$coefficients

summary(fit2)$r.squared
```

We observe $\sqrt{\frac{\lambda_1}{\lambda_p}} \geq 30$ which indicates that there's collinearity in the variables. The variance inflation factor for some of the variables also indicates the presence of collinearity. Thus, ridge and lasso regression will be effective techniques to help us understand our data.

## Finding best lambda

```{r finding the best lambda, fig.align='center', out.width = "70%"}
set.seed(69)

y <- houseData$median_house_value
x <- scale(data.matrix(houseData[ ,-c(1:3, 10, 12)]))

# perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(x, y, alpha = 1)

# find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min

# produce plot of test MSE by lambda value
plot(cv_model)
```

Our plot shows the test MSE by $\lambda$ value. Using cross-validation, we obtain that our best $\lambda$ value is `r best_lambda`. 

### Ridge Regression
We use the `MASS` package to perform ridge regression. Our regression coefficients (after normalization) should not be very large, so that we should bound/restrict the size of the coefficients (shrinkage). 

```{r ridge regression}
require(MASS) 
par(mar = c(2, 2, 0.5, 0.5))

houseData[,-c(1:3, 11:12)] <- scale((houseData[,-c(1:3, 11:12)]), center = T, scale = F)
houseData[, -c(1:3, 12)] <- data.frame(houseData[,-c(1:3, 11:12)], houseData[, 11])

rgmod <- lm.ridge(median_house_value ~., houseData[, -c(1:3, 12)], lambda = seq(0, 100, len = 100))
```

We plot the Ridge Regression model coefficients for each value of $\lambda$.
```{r, fig.align='center', out.width = "70%"}
matplot(rgmod$lambda, coef(rgmod), type = "l", 
        xlab = "lambda", ylab = "Beta hat", 
        title = "Ridge Regression Coefficients", cex = 0.8)
```

Using ridge regression, we obtain the coefficients for a fitted linear model: 
```{r coefficients of ridge regression}
coef(rgmod)[which.min(rgmod$GCV), ]
```

### LASSO Regression
We proceed with performing a LASSO regression. Using the `glmnet` package, we find a best $\lambda$ value to fit our model using Lasso Regression. 

```{r lasso, fig.align='center', out.width = "55%"}
houseData <- read.csv("~/Desktop/Projects/PSTAT 126/Project/ProjectStep1/houseData.csv")

require(glmnet)
y <- scale(houseData$median_house_value)
x <- cbind(scale(model.matrix(median_house_value ~ -1 + ., houseData[, -c(1:3, 11:12)])), model.matrix(median_house_value ~ -1 + ocean_proximity, houseData[, -c(1:3)]))

cv_model <- cv.glmnet(x, y, alpha = 1)

# find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min

par(mar = c(7, 4, 2.2, 0.5))
plot(cv_model)
```
Using Lasso regression, we obtain the coefficients for a fitted linear model: 
```{r coefficients}
# find coefficients of best model
best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(best_model)
```

```{r, fig.align='center', out.width = "55%", eval = F}
# We plot the coefficients of the Lasso regression model:
par(mar = c(7, 4, 0.5, 0.5))
plot(coef(best_model), type="h",
     xlab = "index", ylab = "Coefficient", col = "blue")
```
We obtain a $\lambda$ value of `r best_lambda`. Further, `ocean_proximity<1H OCEAN` is not shown as a coefficient because the lasso regression shrunk it all the way to zero. This means it was completely dropped from the model because it wasn’t influential enough. 
Ridge regression shrinks all coefficients towards zero, but lasso regression has the potential to remove predictors from the model by shrinking the coefficients completely to zero.

### MLR, RR, LASSO Visualization
We construct a single graph that superimposes the three different predictions.
We reintroduce the MLR model that we deduced from *Project Step 3*, using backward selection
```{r MLR model, echo = T, results = "hide"}
fitMLR <- lm(median_house_value ~ housing_median_age + total_rooms + 
    total_bedrooms + population + households + median_income + 
    ocean_proximity + housing_median_age:total_rooms + housing_median_age:population + 
    housing_median_age:households + total_rooms:total_bedrooms + 
    total_rooms:population + total_bedrooms:population + total_bedrooms:households + 
    total_bedrooms:median_income + population:median_income + 
    median_income:ocean_proximity, data = houseData[, -c(1:3, 12)])
```

The coefficients for the fitted MLR model are: 
```{r}
coefficients(fitMLR)
```

We create a visualization with the observed median house values on our x-axis, and predicted mean house values on the y-axis. We superimpose the three different predicted values that we yield: our MLR model, Ridge Regression model, and Lasso Regression model.

```{r superimpose visualization, results = "hide", fig.align='center', out.width = "95%"}
# Observed response variable (x)
houseData$median_house_value

# Predicted response variable (y) 
# MLR
y1 <- predict(fitMLR)

# Ridge
# NOTE: this is the code we use to change our cateogries into dummy
ifelse(houseData$ocean_proximity == "INLAND", 1, 0)
ifelse(houseData$ocean_proximity == "NEAR BAY", 1, 0)
ifelse(houseData$ocean_proximity == "NEAR OCEAN", 1, 0)

y2 <- as.matrix(cbind(const=1, houseData[, -c(1:3, 10, 11:12)], ifelse(houseData$ocean_proximity == "INLAND", 1, 0), ifelse(houseData$ocean_proximity == "NEAR BAY", 1, 0), ifelse(houseData$ocean_proximity == "NEAR OCEAN", 1, 0))) %*%
  as.matrix(coef(rgmod)[which.min(rgmod$GCV), ])

# Lasso
#y3 <- as.matrix(cbind(const=1, houseData[, -c(1:3, 10, 11:12)], model.matrix(~-1+ocean_proximity, houseData))) %*% coef(best_model)

y3 <- predict(best_model, newx = model.matrix(median_house_value ~ ., houseData[, -c(1:3, 12)]), s = best_lambda)

# we create a dataframe with our observed and three types of predicted values 
data.frame(x = houseData$median_house_value, y1 =y1, y2 = y2, y3 = y3[,1]) %>%
  pivot_longer(cols = c(y1, y2, y3))

ggplot(data.frame(x = houseData$median_house_value, y1 =y1, y2 = y2, y3 = y3[,1]) %>%
  pivot_longer(cols = c(y1, y2, y3))) +
  geom_line(aes(x = x, y = value, color = name), alpha = 0.85) +
  scale_color_discrete("", labels=c('MLR', 'Ridge', 'Lasso')) + 
  labs(title = str_wrap("Superimposed Visualization of Observed Median House Values vs Predicted Median House Values", 70), x = "Observed", y= "Predicted") + 
  theme_minimal()
```
Looking at the graph of the three different model predictions superimposed, we see that the MLR and Ridge regression models are very similar while the Lasso regression model’s predictions are significantly smaller than both of the other models. This is further reinforced by the comparison of the coefficients for the models, where it’s evident that the coefficients in the lasso model are significantly smaller than its ridge regression counterparts. Additionally, we can see the similarity between the MLR and Ridge Regression reflected in the comparison between their coefficients. The distinction of the Lasso regression (apart from the other two models) may be caused due to inadequate scaling of the dataset. 

### Innovation: Random Forests
The technique we chose to learn for the project was using random forests to predict observation values. Random forests are appropriate when it comes to predicting observation values, especially when the observation variable is continuous. Additionally, random forests have high accuracy, often better than linear regression models.

Random forests consist of a large quantity of decision trees that use a specified number of random vectors and a specified number of features from the data. Decision trees are a tree structure with levels of nodes to determine an estimate of the observation using the nodes and random vectors (decisions in the tree are made as a result of evaluating the criterion at each node). Each tree generates an output, and the average of the outputs is returned as the final output or predicted value for the observation.

Random forests are beneficial in analysis because of their general ease-of-use and lack of technical conditions required for the model. We found very little pushback when making the model, so we didn't have to react to conditions being violated.

Using a random forest on our variables of interest, we graphed the error of each tree and a comparison of the observed value from the original data and the predicted value for the forest.

```{r, random forests, fig.align='center', out.width = "80%"}
house.rf <- randomForest(median_house_value ~ ., data = houseData[,-c(1:3,12)], mtry = 5,
                         importance = TRUE, na.action = na.omit, ntree = 1000, nPerm = 1, keep.inbag = TRUE, mse = TRUE, rsq = TRUE)

predictions <- predict(house.rf, houseData[,-c(1:3,12)])

# Plot the error vs the number of trees graph 

# Assuming 'house.rf' is your random forest model
mse_data <- data.frame(TreeNumber = 1:house.rf$ntree, MSE = as.vector(house.rf$mse))

# Plot the error vs the number of trees graph 
ggplot(mse_data, aes(x = TreeNumber, y = MSE)) +
  geom_line() +
  labs(x = "Number of Trees", y = "Mean Squared Error", 
       title = "MSE for each tree") +
  theme_minimal()

# Plotting observed vs fitted value
pred_data <- data.frame(observed = houseData$median_house_value, predicted = predictions)

ggplot(pred_data, aes(x = observed, y = predicted)) +
  geom_point() +
  labs(x = 'Observed Value', y = 'Predicted Value', 
       title = 'Observed vs. Predicted Observations') +
  theme_minimal()
```

We visualize our random forest onto our superimposed visualization from before:

```{r final visual, fig.align='center', out.width = "90%"}
ggplot(data.frame(x = houseData$median_house_value, y1 =y1, y2 = y2, y3 = y3[,1], y4 = pred_data$predicted) %>%
  pivot_longer(cols = c(y1, y2, y3, y4))) +
  geom_line(aes(x = x, y = value, color = name), alpha = 0.85) +
  scale_color_discrete("", labels=c('MLR', 'Ridge', 'Lasso', 'Random Forest')) + 
  labs(title = str_wrap("Superimposed Visualization of Observed Median House Values vs Predicted Median House Values", 70), x = "Observed", y= "Predicted") + 
  theme_minimal()
```
We observe that the Random Model produces similar predicted values as do the MLR backwards selection and ridge regression. 

### Conclusion
We conclude that lasso regression was less effective than our backwards MLR selection, ridge regression, and random forest models, for our dataset of California housing. If we were to repeat this analysis, we would try to find data where the number of predictors was larger than the number of observations so that lasso regression could be properly utilized and appreciated. Additionally, we would further explore how to change the random forest model to improve predictive accuracy and analyze other statistics about the random forest model.